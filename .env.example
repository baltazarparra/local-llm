# Local LLM Configuration
# Copy this file to .env and adjust values as needed

# Model Settings
# Available models:
# - Qwen/Qwen2.5-1.5B-Instruct (lightweight, ~3GB VRAM)
# - Qwen/Qwen2.5-7B-Instruct (larger, ~14GB VRAM)
# - meta-llama/Llama-3.2-1B-Instruct
# - microsoft/Phi-3.5-mini-instruct
MODEL_ID=Qwen/Qwen2.5-1.5B-Instruct

# Device and Compute Settings
USE_GPU=true
TORCH_DTYPE=auto  # Options: auto, float16, bfloat16, float32
DEVICE_MAP=auto   # Options: auto, cpu, cuda

# Generation Parameters (Defaults)
MAX_NEW_TOKENS=512
TEMPERATURE=0.7
TOP_P=0.9
TOP_K=50
DO_SAMPLE=true

# API Server Settings
API_HOST=0.0.0.0
API_PORT=8000

# Logging
LOG_LEVEL=INFO  # Options: DEBUG, INFO, WARNING, ERROR
