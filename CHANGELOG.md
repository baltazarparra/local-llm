# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Planned
- Comprehensive test suite (pytest)
- API streaming endpoints (Server-Sent Events)
- Docker containerization
- CI/CD pipeline (GitHub Actions)
- Model quantization support (4-bit/8-bit)

## [0.1.0] - 2025-11-15

### Added
- **Core Engine**
  - Model loading from Hugging Face with automatic downloads
  - Text generation with configurable sampling parameters
  - Streaming generation support via TextIteratorStreamer
  - Chat template support for multi-turn conversations
  - GPU/CPU auto-detection and device mapping
  - Configuration management via environment variables

- **Classic CLI Interface**
  - Interactive REPL-based chat
  - Conversation history (up to 50 messages)
  - Commands: `/help`, `/exit`, `/reset`, `/history`
  - System prompt support
  - Stateless mode option

- **Enhanced Rich UI Interface** 
  - Markdown rendering with syntax highlighting (200+ languages)
  - Live streaming responses with visual feedback
  - Multiline input support (Meta+Enter / Alt+Enter)
  - Persistent command history with Ctrl+R search
  - Auto-suggestions from previous prompts
  - Export conversations to markdown (`/export`)
  - Beautiful panels and formatting with emoji indicators

- **REST API Server**
  - FastAPI-based HTTP server
  - 5 endpoints: `/`, `/health`, `/info`, `/generate`, `/chat`
  - Pydantic request/response validation
  - Actual token counting (not approximations)
  - Error handling with proper HTTP status codes
  - Performance timing in responses

- **One-off Generation Script**
  - Command-line text generation
  - Stdin/stdout pipe support
  - Verbose mode and prompt display options
  - All generation parameters configurable

- **Convenience Features**
  - Shell launchers (`chat.sh`, `chat-rich.sh`, `api.sh`, `run.sh`)
  - Virtual environment auto-activation support
  - direnv configuration (`.envrc`)
  - Comprehensive `.gitignore`
  - MIT License

- **Documentation**
  - Comprehensive README.md with all usage examples
  - Development roadmap in PLAN.md
  - This CHANGELOG.md
  - Code examples and API documentation

### Changed
- Switched from `torch_dtype` to `dtype` parameter (deprecated warning fix)
- Improved token counting from character-based approximation to actual tokenization
- Optimized dependencies (removed unused `torchvision`, `torchaudio`)

### Removed
- Unused `JSONResponse` import from `api_server.py`
- Unused dependencies: `torchvision`, `torchaudio`
- All deprecation warnings

### Fixed
- Token counting now accurate (was character/4 approximation)
- Deprecation warning for `torch_dtype` parameter
- Import optimization (removed unused imports)

### Technical Details
- **Lines of Code**: ~1,600
- **Python Files**: 8
- **Classes**: 9 (100% documented)
- **Functions**: 30+ (100% documented)
- **Supported Models**: Qwen, Llama, Phi-3, and any HuggingFace chat model
- **Default Model**: Qwen/Qwen2.5-1.5B-Instruct (1543.71M parameters)

### Performance
- Model loading: ~2-3 seconds (CPU), ~5 seconds (GPU first time)
- Text generation: ~3 seconds for ~10 tokens (CPU, bfloat16)
- API response time: ~2.9 seconds (CPU)
- Streaming: Real-time token display with 10 FPS refresh

---

## [0.0.1] - 2025-11-14

### Added
- Initial project structure
- Virtual environment setup
- Requirements.txt with dependencies
- Basic configuration skeleton

---

## Version Naming Convention

- **Major (X.0.0)**: Breaking changes, major feature additions
- **Minor (0.X.0)**: New features, non-breaking changes
- **Patch (0.0.X)**: Bug fixes, optimizations

---

## Release Notes

### v0.1.0 Highlights

This is the first functional release of llm-local, featuring:

ðŸŽ¯ **Complete Core Functionality**: All Phase 0-4 features implemented and tested
âœ¨ **4 Interfaces**: Multiple ways to interact with local LLMs
ðŸŽ¨ **Beautiful UX**: Claude Code-like terminal interface with markdown rendering
âš¡ **Streaming**: Real-time token generation with visual feedback
ðŸš€ **Production Ready**: FastAPI server with validation and error handling
ðŸ”’ **Privacy First**: 100% local execution, no cloud dependencies

Perfect for developers, researchers, and anyone wanting local LLM capabilities without relying on cloud services.

---

**Contributors**: llm-local team  
**License**: MIT
