"""
Personal Assistant Chat Interface with Memory.

Enhanced chat interface that automatically stores conversations,
extracts metadata, and retrieves relevant context for intelligent responses.
"""

import argparse
import atexit
import logging
import signal
import sys
import threading
import uuid
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timedelta

from prompt_toolkit import PromptSession
from prompt_toolkit.auto_suggest import AutoSuggestFromHistory
from prompt_toolkit.history import FileHistory
from prompt_toolkit.key_binding import KeyBindings
from rich.console import Console
from rich.live import Live
from rich.markdown import Markdown
from rich.panel import Panel
from rich.table import Table
from rich.text import Text

from .config import config
from .database import AssistantDatabase
from .enrichment import MetadataExtractor
from .generator import TextGenerator
from .google_calendar import GoogleCalendarIntegration
from .health_monitor import HealthMonitor
from .model_loader import get_model_info, load_tokenizer_and_model
from .prompts import (
    ASSISTANT_SYSTEM_PROMPT,
    create_context_injection_prompt,
    create_search_results_summary,
    create_stats_summary,
    create_timeline_summary,
)
from .retrieval import ContextRetriever
from .semantic_search import embedding_to_bytes, generate_embedding

logger = logging.getLogger(__name__)


class AssistantChatCLI:
    """Personal assistant chat interface with memory and context retrieval."""

    def __init__(
        self,
        generator: TextGenerator,
        db_path: str = "assistant_memory.db",
        max_history: int = 50,
        history_file: str = ".assistant_chat_history",
        session_id: str | None = None,
    ):
        """
        Initialize the assistant chat interface.

        Args:
            generator: TextGenerator instance
            db_path: Path to SQLite database
            max_history: Maximum messages in working memory
            history_file: Path for prompt history
            session_id: Optional session identifier
        """
        self.generator = generator
        self.max_history = max_history
        self.history: list[dict[str, str]] = []
        self.session_id = session_id or str(uuid.uuid4())

        # Shutdown flag for graceful exit
        self._shutdown_requested = False

        # Rich console for beautiful output
        self.console = Console()

        # Database for persistent memory
        self.db = AssistantDatabase(db_path)
        self.console.print(f"[cyan]Memory loaded: {db_path}[/cyan]")

        # Metadata extractor
        self.extractor = MetadataExtractor(generator)

        # Context retriever
        self.retriever = ContextRetriever(self.db)

        # Thread pool for background enrichment tasks (bounded to prevent resource exhaustion)
        self.enrichment_executor = ThreadPoolExecutor(
            max_workers=4, thread_name_prefix="enrichment"
        )

        # Health monitoring for long-running operations
        self.health_monitor = HealthMonitor(db_path=db_path, check_interval=300)
        self.health_monitor.start_monitoring()

        # Register cleanup on exit
        atexit.register(self._cleanup)

        # Google Calendar integration
        self.calendar = None
        try:
            self.calendar = GoogleCalendarIntegration(self.db)
            # Try to authenticate (will use cached tokens if available)
            if self.calendar.authenticate():
                self.console.print("[green]ðŸ“… Google Calendar connected[/green]")

                # Check for upcoming events in next 48 hours
                upcoming_events = self.calendar.get_upcoming_events(hours=48)
                if upcoming_events:
                    formatted_events = self.calendar.format_upcoming_events(
                        upcoming_events
                    )
                    self.console.print(
                        Panel(
                            f"ðŸ“… Upcoming Events (Next 48 Hours)\n\n{formatted_events}",
                            title="[bold cyan]Your Schedule[/bold cyan]",
                            border_style="cyan",
                            padding=(1, 2),
                        )
                    )
                    self.console.print()
            else:
                self.console.print(
                    "[yellow]ðŸ“… Google Calendar not configured (use GOOGLE_CALENDAR_SETUP.md)[/yellow]"
                )
                self.calendar = None
        except Exception as e:
            logger.debug(f"Calendar integration not available: {e}")
            self.calendar = None

        # Prompt Toolkit session
        kb = KeyBindings()

        @kb.add("escape", "enter")
        def _(event):
            """Submit on Meta+Enter (ESC+Enter)"""
            event.current_buffer.validate_and_handle()

        # Limit file history size to prevent unbounded growth
        from prompt_toolkit.history import FileHistory as BaseFileHistory

        class LimitedFileHistory(BaseFileHistory):
            """File history with size limit."""

            def __init__(self, filename, max_entries=1000):
                super().__init__(filename)
                self.max_entries = max_entries
                self._trim_history()

            def _trim_history(self):
                """Trim history file if it exceeds max entries."""
                try:
                    with open(self.filename, "r", encoding="utf-8") as f:
                        lines = f.readlines()

                    if len(lines) > self.max_entries:
                        with open(self.filename, "w", encoding="utf-8") as f:
                            f.writelines(lines[-self.max_entries :])
                except FileNotFoundError:
                    pass

        self.session = PromptSession(
            history=LimitedFileHistory(history_file, max_entries=1000),
            auto_suggest=AutoSuggestFromHistory(),
            multiline=True,
            enable_history_search=True,
            key_bindings=kb,
        )

        # Add system prompt to working history
        self.history.append({"role": "system", "content": ASSISTANT_SYSTEM_PROMPT})

    def add_message(self, role: str, content: str):
        """Add a message to working memory."""
        self.history.append({"role": role, "content": content})

        # Trim history if too long (keep system prompt)
        if len(self.history) > self.max_history:
            self.history = [self.history[0]] + self.history[-(self.max_history - 1) :]

            # Trigger garbage collection after trimming
            import gc

            gc.collect()

    def save_conversation(self, role: str, content: str) -> int:
        """
        Save a conversation message to the database.

        Args:
            role: Message role
            content: Message content

        Returns:
            Conversation ID
        """
        conv_id = self.db.add_conversation(
            role=role, content=content, session_id=self.session_id
        )
        logger.debug(f"Saved conversation {conv_id}")
        return conv_id

    def enrich_conversation(self, conversation_id: int, message: str, role: str):
        """
        Extract and save metadata for a conversation asynchronously.

        Args:
            conversation_id: ID of the conversation
            message: Message content
            role: Message role
        """
        try:
            # Extract metadata using LLM
            metadata = self.extractor.extract_metadata(message, role)

            # Generate embedding
            embedding = generate_embedding(message)
            embedding_bytes = embedding_to_bytes(embedding)

            # Save metadata to database
            self.db.add_metadata(
                conversation_id=conversation_id,
                people=metadata.get("people"),
                topics=metadata.get("topics"),
                dates_mentioned=metadata.get("dates_mentioned"),
                sentiment=metadata.get("sentiment"),
                category=metadata.get("category"),
                embedding=embedding_bytes,
            )

            logger.debug(f"Enriched conversation {conversation_id}")
        except Exception as e:
            logger.warning(f"Could not enrich conversation: {e}")

    def _enrich_messages_background(
        self,
        user_conv_id: int,
        user_message: str,
        assistant_conv_id: int,
        assistant_message: str,
    ):
        """
        Enrich both user and assistant messages in background thread.

        This runs asynchronously to avoid blocking the UI while metadata
        extraction and embedding generation happen.

        Args:
            user_conv_id: User conversation ID
            user_message: User message content
            assistant_conv_id: Assistant conversation ID
            assistant_message: Assistant message content
        """
        import time

        # Skip enrichment if shutdown is in progress
        if self._shutdown_requested:
            logger.debug("Skipping enrichment due to shutdown")
            return

        # Retry with exponential backoff for database lock contention
        max_retries = 3
        base_delay = 0.5  # seconds

        for attempt in range(max_retries):
            try:
                self.enrich_conversation(user_conv_id, user_message, "user")
                self.enrich_conversation(
                    assistant_conv_id, assistant_message, "assistant"
                )
                return  # Success, exit
            except Exception as e:
                if "database is locked" in str(e):
                    if self._shutdown_requested:
                        # During shutdown, database locks are expected - suppress warning
                        logger.debug(f"Database locked during shutdown (attempt {attempt + 1})")
                        return
                    elif attempt < max_retries - 1:
                        delay = base_delay * (2**attempt)  # Exponential backoff
                        logger.debug(
                            f"Database locked, retrying in {delay}s (attempt {attempt + 1}/{max_retries})"
                        )
                        time.sleep(delay)
                    else:
                        logger.warning(f"Could not enrich conversation: database is locked")
                        return
                else:
                    logger.warning(f"Background enrichment failed: {e}")
                    return

    def retrieve_context(self, query: str, top_k: int = 5) -> list[Dict]:
        """
        Retrieve relevant context for a query.

        Args:
            query: User query
            top_k: Number of results

        Returns:
            List of relevant conversations
        """
        try:
            contexts = self.retriever.smart_retrieve(query, top_k=top_k)
            logger.debug(f"Retrieved {len(contexts)} relevant contexts")
            return contexts
        except Exception as e:
            logger.warning(f"Could not retrieve context: {e}")
            return []

    def reset_history(self):
        """Clear working memory (keep system prompt)."""
        self.history = [{"role": "system", "content": ASSISTANT_SYSTEM_PROMPT}]

    def print_welcome(self):
        """Display welcome message."""
        stats = self.db.get_stats()

        welcome_text = f"""
# ðŸ§  Personal Assistant with Memory

**Model**: {config.MODEL_ID}
**Device**: {config.get_device()}
**Memory**: {stats["total_conversations"]} conversations stored

I'm your personal work assistant. I remember everything you tell me and can:
- Recall past conversations and context
- Track people, topics, and events
- Provide advice based on your history
- Search through your past interactions

## How to Use

- Type naturally about your work, meetings, tasks, etc.
- Press **Meta+Enter** (ESC then Enter) or **Alt+Enter** to submit
- Press **Ctrl+R** to search command history
- Type `/help` for available commands

I automatically save everything and retrieve relevant context for you.

---
        """
        welcome_panel = Panel(
            Markdown(welcome_text),
            title="[bold blue]Welcome[/bold blue]",
            border_style="blue",
            padding=(1, 2),
        )
        self.console.print(welcome_panel)
        self.console.print()

    def print_help(self):
        """Display help."""
        help_text = """
# ðŸ“– Assistant Commands

| Command | Description |
|---------|-------------|
| `/help`, `/h` | Show this help message |
| `/reset`, `/clear` | Clear working memory (database preserved) |
| `/search <query>` | Search past conversations |
| `/timeline [person]` | Show chronological history |
| `/people` | List all people mentioned |
| `/topics` | List all topics discussed |
| `/stats` | Show memory statistics |
| `/export [file]` | Export current session to markdown |
| `/exit`, `/quit`, `/q` | Exit the assistant |

# âŒ¨ï¸ Keyboard Shortcuts

- **Meta+Enter** (ESC+Enter) or **Alt+Enter** - Submit message
- **Ctrl+R** - Search command history
- **Ctrl+C** - Cancel current input

# ðŸ’¡ Tips

- Speak naturally: "I have a meeting with Sarah tomorrow"
- I automatically remember people, topics, dates, and sentiment
- Ask for advice: "What should I discuss with Sarah?"
- Search history: "/search sarah meeting"
        """
        help_panel = Panel(
            Markdown(help_text),
            title="[bold green]Help[/bold green]",
            border_style="green",
            padding=(1, 2),
        )
        self.console.print(help_panel)
        self.console.print()

    def handle_command(self, command: str) -> bool:
        """
        Handle special commands.

        Args:
            command: Command string

        Returns:
            True if should exit
        """
        parts = command.lower().strip().split(maxsplit=1)
        cmd = parts[0]
        args = parts[1] if len(parts) > 1 else None

        if cmd in ["/exit", "/quit", "/q"]:
            self.console.print(
                "\n[yellow]ðŸ‘‹ Goodbye! Your memory has been saved.[/yellow]\n"
            )
            return True

        elif cmd in ["/reset", "/clear"]:
            self.reset_history()
            self.console.print(
                "[green]âœ“ Working memory cleared (database preserved)[/green]\n"
            )
            return False

        elif cmd in ["/help", "/h"]:
            self.print_help()
            return False

        elif cmd == "/search":
            if not args:
                self.console.print("[red]Usage: /search <query>[/red]\n")
            else:
                self._handle_search(args)
            return False

        elif cmd == "/timeline":
            self._handle_timeline(args)
            return False

        elif cmd == "/people":
            self._handle_people()
            return False

        elif cmd == "/topics":
            self._handle_topics()
            return False

        elif cmd == "/stats":
            self._handle_stats()
            return False

        elif cmd == "/export":
            filename = args if args else "assistant_session.md"
            self._export_session(filename)
            return False

        else:
            self.console.print(f"[red]âœ— Unknown command: {cmd}[/red]")
            self.console.print("Type [green]/help[/green] for available commands\n")
            return False

    def _handle_search(self, query: str):
        """Handle search command."""
        self.console.print(f"[cyan]Searching for: {query}[/cyan]\n")

        results = self.retrieve_context(query, top_k=10)

        if results:
            summary = create_search_results_summary(results, query)
            panel = Panel(
                Markdown(summary),
                title="[bold cyan]Search Results[/bold cyan]",
                border_style="cyan",
                padding=(1, 2),
            )
            self.console.print(panel)
        else:
            self.console.print("[yellow]No results found.[/yellow]")

        self.console.print()

    def _handle_timeline(self, person: str | None):
        """Handle timeline command."""
        if person:
            self.console.print(f"[cyan]Timeline for: {person}[/cyan]\n")
            conversations = self.retriever.retrieve_timeline(person=person, limit=30)
        else:
            self.console.print("[cyan]Recent timeline[/cyan]\n")
            conversations = self.retriever.retrieve_timeline(limit=30)

        if conversations:
            summary = create_timeline_summary(conversations, person)
            panel = Panel(
                Markdown(summary),
                title="[bold cyan]Timeline[/bold cyan]",
                border_style="cyan",
                padding=(1, 2),
            )
            self.console.print(panel)
        else:
            self.console.print("[yellow]No conversations found.[/yellow]")

        self.console.print()

    def _handle_people(self):
        """Handle people command."""
        people = self.db.get_all_people()

        if people:
            table = Table(
                title="People Mentioned", show_header=True, header_style="bold magenta"
            )
            table.add_column("Name", style="cyan")

            for person in people:
                table.add_row(person)

            self.console.print(table)
        else:
            self.console.print("[yellow]No people mentioned yet.[/yellow]")

        self.console.print()

    def _handle_topics(self):
        """Handle topics command."""
        topics = self.db.get_all_topics()

        if topics:
            table = Table(
                title="Topics Discussed", show_header=True, header_style="bold magenta"
            )
            table.add_column("Topic", style="cyan")

            for topic in topics:
                table.add_row(topic)

            self.console.print(table)
        else:
            self.console.print("[yellow]No topics tracked yet.[/yellow]")

        self.console.print()

    def _handle_stats(self):
        """Handle stats command."""
        stats = self.db.get_stats()
        summary = create_stats_summary(stats)

        panel = Panel(
            Markdown(summary),
            title="[bold cyan]Memory Statistics[/bold cyan]",
            border_style="cyan",
            padding=(1, 2),
        )
        self.console.print(panel)
        self.console.print()

    def _export_session(self, filename: str):
        """Export current session to markdown."""
        try:
            with open(filename, "w") as f:
                f.write(f"# Assistant Session\n\n")
                f.write(f"**Session ID**: {self.session_id}\n")
                f.write(f"**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n")
                f.write("---\n\n")

                for msg in self.history:
                    if msg["role"] == "system":
                        continue
                    role = msg["role"].title()
                    content = msg["content"]
                    f.write(f"## {role}\n\n{content}\n\n")

            self.console.print(f"[green]âœ“ Session exported to {filename}[/green]\n")
        except Exception as e:
            self.console.print(f"[red]âœ— Error exporting: {e}[/red]\n")

    def stream_response_with_context(self, user_message: str) -> str:
        """
        Generate response with context retrieval.

        Args:
            user_message: User's message

        Returns:
            Generated response
        """
        # Keep spinner active through entire thinking process
        accumulated = ""
        with self.console.status("[dim]Thinking...", spinner="dots"):
            # Retrieve relevant context
            relevant_contexts = self.retrieve_context(user_message, top_k=5)

            # Prepare messages with context injection
            messages_for_llm = [self.history[0]]  # System prompt

            # If we have relevant context, inject it
            if relevant_contexts:
                context_message = create_context_injection_prompt(
                    relevant_contexts, user_message
                )
                messages_for_llm.append({"role": "user", "content": context_message})
            else:
                # No context, use original message
                messages_for_llm.append({"role": "user", "content": user_message})

            # Add recent conversation history (excluding system prompt)
            for msg in self.history[1:]:
                if msg not in messages_for_llm:
                    messages_for_llm.append(msg)

            # Start streaming and get first chunk before spinner ends
            stream_iterator = self.generator.generate_chat_stream(messages_for_llm)
            try:
                first_chunk = next(stream_iterator)
                accumulated = first_chunk
            except StopIteration:
                return ""

        # Spinner ends here - show context count if relevant
        if relevant_contexts:
            self.console.print(
                f"[dim]Found {len(relevant_contexts)} relevant past conversation(s)[/dim]\n"
            )

        # Now start Live() display with first chunk already visible
        try:
            with Live(refresh_per_second=10, console=self.console) as live:
                # Show first chunk immediately
                md = Markdown(accumulated, code_theme="monokai")
                panel = Panel(
                    md,
                    title="[bold green]ðŸ§  Assistant[/bold green]",
                    border_style="green",
                    padding=(1, 2),
                )
                live.update(panel)

                # Continue with remaining chunks
                for chunk in stream_iterator:
                    accumulated += chunk

                    # Render as markdown
                    md = Markdown(accumulated, code_theme="monokai")
                    panel = Panel(
                        md,
                        title="[bold green]ðŸ§  Assistant[/bold green]",
                        border_style="green",
                        padding=(1, 2),
                    )
                    live.update(panel)

            # Add newline after Live() context for clean console state
            self.console.print()

            # Clear torch cache to prevent memory buildup
            try:
                import torch

                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                # Also trigger garbage collection
                import gc

                gc.collect()
            except Exception:
                pass

            return accumulated

        except Exception as e:
            logger.error(f"Streaming error: {e}")
            self.console.print(f"[red]âœ— Error during generation: {e}[/red]")
            return ""

    def print_ready_indicator(self):
        """Display ready indicator showing assistant is waiting for input."""
        from rich.rule import Rule

        # Ensure clean separation from previous output
        self.console.print()
        self.console.print(
            Rule("[dim green]Ready[/dim green]", style="dim green", characters="Â·")
        )
        self.console.print(
            "[dim]Type your message and press Meta+Enter (ESC+Enter) or Alt+Enter to send[/dim]"
        )
        self.console.print()

    def _cleanup(self):
        """Cleanup resources on exit."""
        logger.info("Cleaning up resources...")

        # Set shutdown flag to prevent new enrichment tasks
        self._shutdown_requested = True

        # Stop health monitoring
        if hasattr(self, "health_monitor"):
            self.health_monitor.stop_monitoring()
            # Log final health summary
            summary = self.health_monitor.get_metrics_summary()
            if summary:
                logger.info(f"Session health summary: {summary}")

        # Shutdown thread pool gracefully
        if hasattr(self, "enrichment_executor"):
            logger.info("Waiting for enrichment tasks to complete...")
            try:
                # Wait for in-flight enrichment to complete
                # Note: shutdown() has no timeout parameter, but we've already set
                # _shutdown_requested flag to prevent new tasks, so this should finish quickly
                self.enrichment_executor.shutdown(wait=True, cancel_futures=False)
                logger.info("Enrichment tasks completed")
            except Exception as e:
                logger.error(f"Error during enrichment shutdown: {e}")
                # Force immediate shutdown without waiting
                self.enrichment_executor.shutdown(wait=False)

        # Close database connection AFTER enrichment threads have stopped
        if hasattr(self, "db"):
            try:
                self.db.close()
                logger.info("Database closed")
            except Exception as e:
                # Suppress database lock errors during shutdown - they're expected
                if "database is locked" not in str(e).lower():
                    logger.error(f"Error closing database: {e}")

    def run(self):
        """Start the interactive assistant chat loop."""
        self.print_welcome()

        while True:
            try:
                # Get user input with styled prompt
                from prompt_toolkit.formatted_text import HTML

                prompt_text = HTML(
                    "<ansiblue><b>You</b></ansiblue> <ansicyan>âžœ</ansicyan> "
                )
                user_input = self.session.prompt(prompt_text, multiline=True).strip()

                # Skip empty input
                if not user_input:
                    continue

                # Handle commands
                if user_input.startswith("/"):
                    should_exit = self.handle_command(user_input)
                    if should_exit:
                        break
                    continue

                # Check for calendar intent before processing
                calendar_event = None
                calendar_created = False
                if self.calendar:
                    calendar_event = self.extractor.detect_calendar_intent(user_input)
                    if calendar_event:
                        # Show loading indicator while creating event
                        from rich.live import Live
                        from rich.spinner import Spinner

                        with Live(
                            Spinner("dots", text="Creating calendar event..."),
                            console=self.console,
                            transient=True,
                        ):
                            # Create calendar event
                            created_event = self.calendar.create_event(
                                summary=calendar_event["summary"],
                                start_time=calendar_event["datetime"],
                                end_time=calendar_event["datetime"]
                                + timedelta(hours=calendar_event["duration_hours"]),
                                description=calendar_event.get("description"),
                            )

                        if created_event:
                            calendar_created = True
                            # Show confirmation
                            confirmation = self.calendar.format_event_confirmation(
                                created_event
                            )
                            self.console.print(
                                Panel(
                                    confirmation,
                                    title="[bold green]Calendar Event Created[/bold green]",
                                    border_style="green",
                                )
                            )
                            self.console.print()

                            # Skip normal response - just save and continue
                            user_conv_id = self.save_conversation("user", user_input)
                            self.add_message("user", user_input)
                            self.print_ready_indicator()
                            continue

                # Save user message to database
                user_conv_id = self.save_conversation("user", user_input)

                # Add to working memory
                self.add_message("user", user_input)

                # Generate response with context
                self.console.print()
                response = self.stream_response_with_context(user_input)

                if response:
                    # Save assistant response
                    assistant_conv_id = self.save_conversation("assistant", response)

                    # Add to working memory
                    self.add_message("assistant", response)

                    # Show ready indicator FIRST (immediate feedback)
                    self.print_ready_indicator()

                    # Enrich both messages in background using thread pool (non-blocking, bounded)
                    self.enrichment_executor.submit(
                        self._enrich_messages_background,
                        user_conv_id,
                        user_input,
                        assistant_conv_id,
                        response,
                    )
                else:
                    # No response, still show ready indicator
                    self.print_ready_indicator()

            except KeyboardInterrupt:
                self.console.print(
                    "\n[yellow]âš  Interrupted. Type /exit to quit or continue chatting.[/yellow]\n"
                )
                continue

            except EOFError:
                self.console.print(
                    "\n[yellow]ðŸ‘‹ Goodbye! Your memory has been saved.[/yellow]\n"
                )
                break


def main():
    """Main entry point for the assistant chat."""
    parser = argparse.ArgumentParser(
        description="Personal Assistant with Memory - ego_proxy"
    )
    parser.add_argument(
        "--model", type=str, default=None, help=f"Model ID (default: {config.MODEL_ID})"
    )
    parser.add_argument(
        "--db",
        type=str,
        default="assistant_memory.db",
        help="Database path (default: assistant_memory.db)",
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=None,
        help=f"Maximum tokens to generate (default: {config.MAX_NEW_TOKENS})",
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=None,
        help=f"Sampling temperature (default: {config.TEMPERATURE})",
    )
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="Show detailed technical logs for debugging",
    )

    args = parser.parse_args()

    # Set verbose mode in config
    config.VERBOSE_MODE = args.verbose

    # Configure logging with Rich handler and file rotation
    from logging.handlers import RotatingFileHandler

    from rich.logging import RichHandler

    log_level = logging.DEBUG if args.verbose else logging.WARNING

    # Create handlers
    handlers = [RichHandler(rich_tracebacks=True, show_time=False, show_path=False)]

    # Add rotating file handler for persistent logs
    file_handler = RotatingFileHandler(
        "assistant.log",
        maxBytes=10 * 1024 * 1024,  # 10MB per file
        backupCount=5,  # Keep 5 backup files
        encoding="utf-8",
    )
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(
        logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    )
    handlers.append(file_handler)

    logging.basicConfig(
        level=log_level,
        format="%(message)s",
        handlers=handlers,
    )

    # Override config if specified
    if args.max_tokens:
        config.MAX_NEW_TOKENS = args.max_tokens
    if args.temperature is not None:
        config.TEMPERATURE = args.temperature

    console = Console()
    assistant = None

    # Set up signal handlers for graceful shutdown
    def signal_handler(signum, frame):
        """Handle SIGTERM and SIGINT for graceful shutdown."""
        console.print("\n[yellow]âš  Shutting down gracefully...[/yellow]")
        if assistant:
            assistant._shutdown_requested = True
            assistant._cleanup()
        # Don't call sys.exit(0) here - it causes threading exceptions
        # Let the program exit naturally after cleanup

    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)

    try:
        # Load model and tokenizer
        with console.status(
            "[cyan]Loading model...", spinner="dots", spinner_style="cyan"
        ):
            tokenizer, model = load_tokenizer_and_model(model_id=args.model)

        # Print model info
        model_info = get_model_info(model)
        if model_info:
            info_text = f"""
**Model loaded**: {model_info.get("num_parameters_millions", "?")}M parameters
**Device**: {model_info.get("device", "?")}
**Dtype**: {model_info.get("dtype", "?")}
            """
            console.print(Panel(Markdown(info_text), border_style="cyan"))
            console.print()

        # Create generator
        generator = TextGenerator(model, tokenizer)

        # Create and run assistant
        assistant = AssistantChatCLI(generator=generator, db_path=args.db)
        assistant.run()

    except KeyboardInterrupt:
        console.print("\n[yellow]âš  Interrupted during startup. Exiting...[/yellow]")
        sys.exit(0)

    except Exception as e:
        logger.error(f"Failed to start assistant: {e}")
        console.print(f"\n[red]âœ— Error: {e}[/red]")
        sys.exit(1)


if __name__ == "__main__":
    main()
